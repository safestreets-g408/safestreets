{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3264b441-0de6-4670-8c70-6b51c71745d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, AutoTokenizer, AutoModelForSeq2SeqLM, AdamW\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fcf2a98-8c80-482f-81e7-405864f60c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinayb/jupyter_env/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e231f85ec44eb1b5c8ead08e3a2090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  39%|###9      | 136M/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2486fc84b4343efabc496df6b9001ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c115e5d55b56430dbd0fd23c30bae6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eff4dd8a1fd46158cd4d3556673c269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c776d151fd4b37ba2a527fd6bfc214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5eebb41caef4fc28a7688aeea24198f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ee70d33f9640cfb8bc1178008ec1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vit_model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "vit_feature_extractor = ViTFeatureExtractor.from_pretrained(vit_model_name)\n",
    "vit_model = ViTForImageClassification.from_pretrained(vit_model_name)\n",
    "bert_summary_model_name = \"facebook/bart-large-cnn\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_summary_model_name)\n",
    "bert_summary_model = AutoModelForSeq2SeqLM.from_pretrained(bert_summary_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae136c4d-9843-45fb-adea-1bbdd2b27309",
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_categories = {\n",
    "    \"D00\": \"Longitudinal Crack\",\n",
    "    \"D10\": \"Transverse Crack\",\n",
    "    \"D20\": \"Alligator Crack\",\n",
    "    \"D40\": \"Pothole\",\n",
    "    \"D50\": \"Surface Cracks\",\n",
    "    \"D60\": \"Erosion/wear\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eab73ea-1969-4db4-a3b6-7cc9ebdd041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadDamageDataset(Dataset):\n",
    "    def __init__(self, image_folder, annotation_folder, feature_extractor, tokenizer, max_length=256): #added max_length\n",
    "        self.image_folder = image_folder\n",
    "        self.annotation_folder = annotation_folder\n",
    "        self.image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length #Added max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_folder, image_file)\n",
    "        annotation_path = os.path.join(self.annotation_folder, os.path.splitext(image_file)[0] + \".xml\")\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        damage_details = []\n",
    "        for obj in root.findall(\"object\"):\n",
    "            label = obj.find(\"name\").text\n",
    "            xmin = int(obj.find(\"bndbox/xmin\").text)\n",
    "            ymin = int(obj.find(\"bndbox/ymin\").text)\n",
    "            xmax = int(obj.find(\"bndbox/xmax\").text)\n",
    "            ymax = int(obj.find(\"bndbox/ymax\").text)\n",
    "            if label in damage_categories:\n",
    "                damage_type = damage_categories[label]\n",
    "            else:\n",
    "                damage_type = \"Unspecified Damage\"\n",
    "\n",
    "            area = (xmax - xmin) * (ymax - ymin)\n",
    "            if area > 10000:\n",
    "                severity = \"High\"\n",
    "                priority = \"Urgent\"\n",
    "                action = \"Immediate repair suggested\"\n",
    "            elif area > 5000:\n",
    "                severity = \"Medium\"\n",
    "                priority = \"High\"\n",
    "                action = \"Repair recommended\"\n",
    "            else:\n",
    "                severity = \"Low\"\n",
    "                priority = \"Moderate\"\n",
    "                action = \"Monitor and schedule repair\"\n",
    "\n",
    "            damage_details.append({\n",
    "                \"type\": damage_type,\n",
    "                \"severity\": severity,\n",
    "                \"priority\": priority,\n",
    "                \"action\": action\n",
    "            })\n",
    "\n",
    "        summary_input_text = f\"The image shows a road. \"\n",
    "        for detail in damage_details:\n",
    "            summary_input_text += f\"Type: {detail['type']}, Severity: {detail['severity']}, Priority: {detail['priority']}, Action: {detail['action']}. \"\n",
    "\n",
    "        summary_inputs = self.tokenizer([summary_input_text], return_tensors=\"pt\", max_length=self.max_length, padding='max_length', truncation=True) #added padding and max_length\n",
    "        return inputs['pixel_values'].squeeze(0), summary_inputs['input_ids'].squeeze(0), summary_inputs['attention_mask'].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce37302e-8050-4d41-b1ad-cb22be7ef6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(vit_model, bert_summary_model, dataset, epochs=3, batch_size=4, learning_rate=1e-5, save_path_vit = \"vit_finetuned.pth\", save_path_bart = \"bart_finetuned.pth\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vit_model.to(device)\n",
    "    bert_summary_model.to(device)\n",
    "    vit_model.train()\n",
    "    bert_summary_model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    vit_optimizer = AdamW(vit_model.parameters(), lr=learning_rate)\n",
    "    bert_optimizer = AdamW(bert_summary_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for pixel_values, summary_input_ids, summary_attention_mask in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "            pixel_values = pixel_values.to(device)\n",
    "            summary_input_ids = summary_input_ids.to(device)\n",
    "            summary_attention_mask = summary_attention_mask.to(device)\n",
    "\n",
    "            vit_outputs = vit_model(pixel_values, labels=vit_model(pixel_values).logits.argmax(dim=-1))\n",
    "            vit_loss = vit_outputs.loss\n",
    "            vit_optimizer.zero_grad()\n",
    "            vit_loss.backward()\n",
    "            vit_optimizer.step()\n",
    "\n",
    "            bert_outputs = bert_summary_model(input_ids=summary_input_ids, attention_mask=summary_attention_mask, labels=summary_input_ids) # corrected line.\n",
    "            bert_loss = bert_outputs.loss\n",
    "            bert_optimizer.zero_grad()\n",
    "            bert_loss.backward()\n",
    "            bert_optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, ViT Loss: {vit_loss.item()}, BART Loss: {bert_loss.item()}\")\n",
    "\n",
    "    torch.save(vit_model.state_dict(), save_path_vit)\n",
    "    torch.save(bert_summary_model.state_dict(), save_path_bart)\n",
    "    print(\"Fine-tuned models saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11394352-d66b-4871-bf20-be4aeed2a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_road_damage(image_path, vit_model, bert_summary_model):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: Image not found at {image_path}\"\n",
    "\n",
    "    inputs = vit_feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = vit_model(inputs['pixel_values'].to(vit_model.device))\n",
    "        logits = outputs.logits\n",
    "        predicted_class_idx = logits.argmax(-1).item()\n",
    "        predicted_class = vit_model.config.id2label[predicted_class_idx]\n",
    "\n",
    "    summary_input_text = f\"The image shows a road with {predicted_class}. \"\n",
    "    inputs = bert_tokenizer([summary_input_text], return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        summary_ids = bert_summary_model.generate(inputs[\"input_ids\"].to(bert_summary_model.device), num_beams=4, max_length=256, early_stopping=True)\n",
    "    summary = bert_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da635737-f721-47bf-b847-f1b63138fc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|                                       | 0/1927 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "image_folder = \"/Users/abhinayb/Downloads/India/train/images\"  # Replace with your image folder path\n",
    "annotation_folder = \"/Users/abhinayb/Downloads/India/train/annotations/xmls\"  # Replace with your annotation folder path\n",
    "\n",
    "dataset = RoadDamageDataset(image_folder, annotation_folder, vit_feature_extractor, bert_tokenizer)\n",
    "fine_tune(vit_model, bert_summary_model, dataset)\n",
    "\n",
    "#Load the finetuned models.\n",
    "vit_model.load_state_dict(torch.load(\"vit_finetuned.pth\", map_location=torch.device('cpu')))\n",
    "bert_summary_model.load_state_dict(torch.load(\"bart_finetuned.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "vit_model.eval()\n",
    "bert_summary_model.eval()\n",
    "\n",
    "image_path = \"/content/India_000071.jpg\" #Replace with your test image path.\n",
    "\n",
    "analysis_result = analyze_road_damage(image_path, vit_model, bert_summary_model)\n",
    "print(analysis_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
